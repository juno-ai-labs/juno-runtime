services:
  # Speech-to-Text Service
  stt:
    build:
      context: .
      dockerfile: Dockerfile-stt
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - PYTORCH_VERSION=2.8.0
        - TORCHAUDIO_VERSION=2.8.0a0+6e1c7fe
      cache_from:
        - ghcr.io/juno-ai-labs/juno-stt:latest
    container_name: helios-stt
    hostname: stt
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access for CUDA acceleration
    runtime: nvidia

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions for audio
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared IPC volume
    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - CUDA_LAUNCH_BLOCKING=1
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "1gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits - allow GPU access for STT
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Networking
    networks:
      - helios-net

    # Depends on shared volume being ready
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Speech-to-Text Stream Service (Kyutai STT-1B)
  stt-stream:
    build:
      context: .
      dockerfile: Dockerfile-stt-stream
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - PYTORCH_VERSION=2.7.0
        - TORCHAUDIO_VERSION=2.8.0a0+6e1c7fe
      cache_from:
        - ghcr.io/juno-ai-labs/juno-stt-stream:latest
    container_name: helios-stt-stream
    hostname: stt-stream
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access for CUDA acceleration
    runtime: nvidia

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions for audio
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared volumes with IPC
    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - CUDA_LAUNCH_BLOCKING=1
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "2gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits - allow GPU access for STT
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Speech-to-Text Stream Service - Rust Implementation (Moshi with VAD)
  stt-stream-rust:
    build:
      context: ./stt-stream-rust
      dockerfile: ../Dockerfile-stt-stream-rust
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
      cache_from:
        - ghcr.io/juno-ai-labs/juno-stt-stream-rust:latest
    container_name: helios-stt-stream-rust
    hostname: stt-stream-rust
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access for CUDA acceleration
    runtime: nvidia

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions for audio
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared volumes with IPC
    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # NVIDIA/CUDA
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda
      # Rust
      - RUST_LOG=info
      - RUST_BACKTRACE=1
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "2gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits - allow GPU access for STT
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy
      audio-manager:
        condition: service_started

  audio-manager:
    build:
      context: .
      dockerfile: Dockerfile-audio-manager
      cache_from:
        - ghcr.io/juno-ai-labs/juno-audio-manager:latest
    container_name: helios-audio-manager
    hostname: audio-manager
    restart: on-failure:3

    devices:
      - /dev/snd:/dev/snd

    group_add:
      - audio

    user: "1000:1000"

    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    environment:
      - PYTHONUNBUFFERED=1
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda

    networks:
      - helios-net

    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  web-server:
    build:
      context: .
      dockerfile: Dockerfile-web-server
      cache_from:
        - ghcr.io/juno-ai-labs/juno-web-server:latest
    container_name: helios-web-server
    hostname: web-server
    restart: on-failure:3

    user: "10000:10000"

    volumes:
      - ipc_sockets:/tmp/ipc

    environment:
      - PYTHONUNBUFFERED=1
      - WEB_SERVER_HOST=0.0.0.0
      - WEB_SERVER_PORT=8080

    ports:
      - "8080:8080"

    networks:
      - helios-net

    depends_on:
      message-broker:
        condition: service_healthy
      audio-manager:
        condition: service_started

  # Large Language Model Service
  llm:
    build:
      context: .
      dockerfile: Dockerfile-llm
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
      cache_from:
        - ghcr.io/juno-ai-labs/juno-llm:latest
    container_name: helios-llm
    hostname: llm
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access
    runtime: nvidia

    # IPC volumes
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONUNBUFFERED=1
      - LLM_SERVER_URL=http://llm-qwen3-4b:8080
      - LLM_SERVER_MODEL=default
      - LLM_SERVER_TIMEOUT_SECONDS=60
      - LLM_SERVER_MAX_RETRIES=3

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy
      llm-qwen3-4b:
        condition: service_healthy

  # Gemma 3 4B llama.cpp HTTP server
  llm-gemma3-4b:
    build:
      context: .
      dockerfile: Dockerfile-llama-cpp-server
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - LLAMA_CPP_RELEASE_TAG=b6937
        - HF_REPO=ggml-org/gemma-3-4b-it-GGUF
        - HF_QUANTIZATION=Q4_K_M
      cache_from:
        - ghcr.io/juno-ai-labs/juno-llm-gemma3-4b:latest
    container_name: helios-llm-gemma3-4b
    hostname: llm-gemma3-4b
    restart: on-failure:3

    runtime: nvidia

    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - SERVICE_NAME=llm-gemma3-4b
      - LLAMA_TEMPERATURE=1.0
      - LLAMA_TOP_K=64
      - LLAMA_TOP_P=0.95
      - LLAMA_MIN_P=0.01
      - LLAMA_REPEAT_PENALTY=1.0
      - LLAMA_LOG_VERBOSITY=0
    ports:
      - "8200:8080"

    volumes:
      - ipc_sockets:/tmp/ipc

    # Resource limits
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    networks:
      - helios-net

    depends_on:
      setup-ipc:
        condition: service_completed_successfully

  # Qwen3 4B llama.cpp HTTP server (primary LLM)
  llm-qwen3-4b:
    build:
      context: .
      dockerfile: Dockerfile-llama-cpp-server
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - LLAMA_CPP_RELEASE_TAG=b6937
        - HF_REPO=unsloth/Qwen3-4B-Instruct-2507-GGUF
        - HF_QUANTIZATION=Q4_K_M
      cache_from:
        - ghcr.io/juno-ai-labs/juno-llm-qwen3-4b:latest
    container_name: helios-llm-qwen3-4b
    hostname: llm-qwen3-4b
    restart: on-failure:3

    runtime: nvidia

    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - SERVICE_NAME=llm-qwen3-4b
      # These are default non-thinking Qwen3 sampling parameters
      - LLAMA_TEMPERATURE=0.7
      - LLAMA_TOP_K=20
      - LLAMA_TOP_P=0.80
      - LLAMA_MIN_P=0.0
      - LLAMA_REPEAT_PENALTY=1.0
      - LLAMA_CTX_SIZE=8192
      - LLAMA_LOG_VERBOSITY=0
    ports:
      - "8201:8080"

    volumes:
      - ipc_sockets:/tmp/ipc

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    networks:
      - helios-net

    depends_on:
      setup-ipc:
        condition: service_completed_successfully

  # Embedding Gemma 300M llama.cpp server
  llm-embedding:
    build:
      context: .
      dockerfile: Dockerfile-llama-cpp-server
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - LLAMA_CPP_RELEASE_TAG=b6937
        - HF_REPO=ggml-org/embeddinggemma-300m-qat-q8_0-GGUF
        - HF_QUANTIZATION=Q8_0
      cache_from:
        - ghcr.io/juno-ai-labs/juno-llm-embedding:latest
    container_name: helios-llm-embedding
    hostname: llm-embedding
    restart: on-failure:3

    runtime: nvidia

    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - SERVICE_NAME=llm-embedding
      - LLAMA_TEMPERATURE=1.0
      - LLAMA_TOP_K=64
      - LLAMA_TOP_P=0.95
      - LLAMA_MIN_P=0.01
      - LLAMA_REPEAT_PENALTY=1.0
      - LLAMA_SERVER_ARGS=--embeddings
      - LLAMA_CTX_SIZE=4096
      - LLAMA_LOG_VERBOSITY=0
    ports:
      - "8202:8080"

    volumes:
      - ipc_sockets:/tmp/ipc

    # Resource limits
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    networks:
      - helios-net

    depends_on:
      setup-ipc:
        condition: service_completed_successfully

  # Text-to-Speech Service
  tts:
    build:
      context: .
      dockerfile: Dockerfile-tts
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - PYTORCH_VERSION=2.8.0
      cache_from:
        - ghcr.io/juno-ai-labs/juno-tts:latest
    container_name: helios-tts
    hostname: tts
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access for CUDA acceleration
    runtime: nvidia

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared volumes
    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "1gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy
      audio-manager:
        condition: service_started

  # IPC Setup Service (ensures socket directory exists with proper permissions)
  setup-ipc:
    image: busybox
    container_name: helios-ipc-setup
    command: >
      sh -c "
        mkdir -p /tmp/ipc &&
        chmod 777 /tmp/ipc &&
        echo 'IPC directory ready'
      "
    volumes:
      - ipc_sockets:/tmp/ipc
    networks:
      - helios-net

  # Message Broker Service (XPUB/XSUB message broker for multi-channel pub/sub)
  message-broker:
    build:
      context: .
      dockerfile: Dockerfile-message-broker
    container_name: helios-message-broker
    hostname: message-broker
    restart: on-failure:3 # Restart on crash, max 3 times
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    environment:
      - PYTHONUNBUFFERED=1
    networks:
      - helios-net
    depends_on:
      setup-ipc:
        condition: service_completed_successfully

  # Monitor Service (containerized Python monitor)
  monitor:
    build:
      context: .
      dockerfile: Dockerfile-monitor
    container_name: helios-monitor
    hostname: monitor
    restart: on-failure:3 # Restart on crash, max 3 times
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - helios-net
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Memory Service (filesystem-based memory persistence)
  memory:
    build:
      context: .
      dockerfile: Dockerfile-memory
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
      cache_from:
        - ghcr.io/juno-ai-labs/juno-memory:latest
    container_name: helios-memory
    hostname: memory
    restart: on-failure:3 # Restart on crash, max 3 times

    # Shared IPC volume for ZeroMQ
    volumes:
      - ipc_sockets:/tmp/ipc
      - memory_data:/data/memory
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      - PYTHONUNBUFFERED=1
      - FILTER_SERVER_URL=http://llm-qwen3-4b:8080 # internal port
      - EMBEDDING_SERVER_URL=http://llm-embedding:8080 # internal port
      - TRANSCRIPT_DEBOUNCE_SECONDS=60
      - REWRITE_INTERVAL_SECONDS=3600
      - REWRITE_WINDOW_DAYS=7

    # Networking
    networks:
      - helios-net

    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy
      llm-qwen3-4b:
        condition: service_started
      llm-embedding:
        condition: service_started

  # CLI Client Service (command line interface for single prompts)
  cli:
    build:
      context: .
      dockerfile: Dockerfile-cli
    container_name: helios-cli
    hostname: cli
    restart: "no" # Don't restart automatically
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - helios-net
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONUNBUFFERED=1

  # Test Playback Service (validates audio setup without GPU)
  test-playback:
    build:
      context: .
      dockerfile: Dockerfile-test-playback
      cache_from:
        - ghcr.io/juno-ai-labs/juno-test-playback:latest
    container_name: helios-test-playback
    hostname: test-playback
    restart: "no" # Don't restart automatically - run once for testing

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions for audio
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared volumes with PulseAudio
    volumes:
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config
      - PYTHONUNBUFFERED=1
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda

    # Networking
    networks:
      - helios-net

# Named volumes
volumes:
  # IPC communication via Unix domain sockets
  ipc_sockets:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: "size=100m,uid=1000,gid=1000,mode=0777"
  memory_data:
    driver: local

# Custom network for service discovery
networks:
  helios-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
