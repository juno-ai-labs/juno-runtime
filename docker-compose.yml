services:
  # Speech-to-Text Service
  stt:
    build:
      context: .
      dockerfile: Dockerfile-stt
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - PYTORCH_VERSION=2.8.0
        - TORCHAUDIO_VERSION=2.8.0a0+6e1c7fe
      cache_from:
        - ghcr.io/juno-ai-labs/juno-stt:latest
    container_name: helios-stt
    hostname: stt
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access for CUDA acceleration
    runtime: nvidia

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions for audio
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared IPC volume
    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - CUDA_LAUNCH_BLOCKING=1
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "1gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits - allow GPU access for STT
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    # Networking
    networks:
      - helios-net

    # Depends on shared volume being ready
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Speech-to-Text Stream Service (Kyutai STT-1B)
  stt-stream:
    build:
      context: .
      dockerfile: Dockerfile-stt-stream
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - PYTORCH_VERSION=2.7.0
        - TORCHAUDIO_VERSION=2.8.0a0+6e1c7fe
      cache_from:
        - ghcr.io/juno-ai-labs/juno-stt-stream:latest
    container_name: helios-stt-stream
    hostname: stt-stream
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access for CUDA acceleration
    runtime: nvidia

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions for audio
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared volumes with IPC
    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - CUDA_LAUNCH_BLOCKING=1
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "2gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits - allow GPU access for STT
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Speech-to-Text Stream Service - Rust Implementation (Moshi with VAD)
  stt-stream-rust:
    build:
      context: ./stt-stream-rust
      dockerfile: ../Dockerfile-stt-stream-rust
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
      cache_from:
        - ghcr.io/juno-ai-labs/juno-stt-stream-rust:latest
    container_name: helios-stt-stream-rust
    hostname: stt-stream-rust
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access for CUDA acceleration
    runtime: nvidia

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions for audio
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared volumes with IPC
    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # NVIDIA/CUDA
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda
      # Rust
      - RUST_LOG=info
      - RUST_BACKTRACE=1
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "2gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits - allow GPU access for STT
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Large Language Model Service
  llm:
    build:
      context: .
      dockerfile: Dockerfile-llm
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - PYTORCH_VERSION=2.8.0
        - LLAMA_CPP_PYTHON_VERSION=0.3.16
      cache_from:
        - ghcr.io/juno-ai-labs/juno-llm:latest
    container_name: helios-llm
    hostname: llm
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access
    runtime: nvidia

    # IPC volumes
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "20gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits - most of GPU memory
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Gemma 3 4B llama.cpp HTTP server
  llm-gemma3-4b:
    build:
      context: .
      dockerfile: Dockerfile-llm-gemma3-4b
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - LLAMA_CPP_RELEASE_TAG=b4062
        - GEMMA_MODEL_QUANT=Q4_K_M
      cache_from:
        - ghcr.io/juno-ai-labs/juno-llm-gemma3-4b:latest
    container_name: helios-llm-gemma3-4b
    hostname: llm-gemma3-4b
    restart: on-failure:3

    runtime: nvidia

    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - LLAMA_SERVER_PORT=8080
    ports:
      - "8200:8080"

    # Resource limits
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    networks:
      - helios-net

    depends_on:
      setup-ipc:
        condition: service_completed_successfully

  # Embedding Gemma 300M llama.cpp server
  llm-embedding:
    build:
      context: .
      dockerfile: Dockerfile-llm-embedding
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - LLAMA_CPP_RELEASE_TAG=b4062
        - EMBEDDING_MODEL_QUANT=embeddinggemma-300m-qat-q8_0.gguf
      cache_from:
        - ghcr.io/juno-ai-labs/juno-llm-embedding:latest
    container_name: helios-llm-embedding
    hostname: llm-embedding
    restart: on-failure:3

    runtime: nvidia

    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - SERVICE_NAME=llm-embedding
      - LLAMA_MODEL_PATH=/models/embedding-gemma/embeddinggemma-300m-qat-q8_0.gguf
      - LLAMA_MODEL_ALIAS=embedding-gemma-300m
      - LLAMA_SERVER_PORT=8080
      - LLAMA_SERVER_ARGS=--embeddings
    ports:
      - "8202:8080"

    # Resource limits
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    networks:
      - helios-net

    depends_on:
      setup-ipc:
        condition: service_completed_successfully

  # Text-to-Speech Service
  tts:
    build:
      context: .
      dockerfile: Dockerfile-tts
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - PYTORCH_VERSION=2.8.0
      cache_from:
        - ghcr.io/juno-ai-labs/juno-tts:latest
    container_name: helios-tts
    hostname: tts
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access for CUDA acceleration
    runtime: nvidia

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared volumes
    volumes:
      - ipc_sockets:/tmp/ipc
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "1gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # IPC Setup Service (ensures socket directory exists with proper permissions)
  setup-ipc:
    image: busybox
    container_name: helios-ipc-setup
    command: >
      sh -c "
        mkdir -p /tmp/ipc &&
        chmod 777 /tmp/ipc &&
        echo 'IPC directory ready'
      "
    volumes:
      - ipc_sockets:/tmp/ipc
    networks:
      - helios-net

  # Message Broker Service (XPUB/XSUB message broker for multi-channel pub/sub)
  message-broker:
    build:
      context: .
      dockerfile: Dockerfile-message-broker
    container_name: helios-message-broker
    hostname: message-broker
    restart: on-failure:3 # Restart on crash, max 3 times
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    environment:
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1
    networks:
      - helios-net
    depends_on:
      setup-ipc:
        condition: service_completed_successfully

  # Monitor Service (containerized Python monitor)
  monitor:
    build:
      context: .
      dockerfile: Dockerfile-monitor
    container_name: helios-monitor
    hostname: monitor
    restart: on-failure:3 # Restart on crash, max 3 times
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - helios-net
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Memory Service (stores and retrieves household memories)
  memory:
    build:
      context: .
      dockerfile: Dockerfile-memory
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
        - PYTORCH_VERSION=2.8.0
        - LLAMA_CPP_PYTHON_VERSION=0.3.16
      cache_from:
        - ghcr.io/juno-ai-labs/juno-memory:latest
    container_name: helios-memory
    hostname: memory
    restart: on-failure:3 # Restart on crash, max 3 times

    # GPU access
    runtime: nvidia

    # IPC volumes
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      # Model loading config
      - LOCAL_MODEL_ONLY=true
    shm_size: "2gb"
    ulimits:
      memlock: -1
      stack: 67108864

    # Resource limits - share GPU with other services
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    # Networking
    networks:
      - helios-net

    # Depends on IPC setup
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy

  # Memory File Service (filesystem-based memory persistence)
  memory-file:
    build:
      context: .
      dockerfile: Dockerfile-memory-file
      args:
        - BASE_IMAGE=ghcr.io/juno-ai-labs/l4t-jetpack:r36.4.0
        - PYTHON_VERSION=310
      cache_from:
        - ghcr.io/juno-ai-labs/juno-memory-file:latest
    container_name: helios-memory-file
    hostname: memory-file
    restart: on-failure:3 # Restart on crash, max 3 times

    # Shared IPC volume for ZeroMQ
    volumes:
      - ipc_sockets:/tmp/ipc
      - memory_data:/data/memory
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1
      - FILTER_SERVER_URL=http://llm-gemma3-4b:8080  # internal port
      - FILTER_MODEL_ALIAS=gemma3-4b-it
      - EMBEDDING_SERVER_URL=http://llm-embedding:8080  # internal port
      - EMBEDDING_MODEL_ALIAS=embedding-gemma-300m
      - TRANSCRIPT_DEBOUNCE_SECONDS=60
      - REWRITE_INTERVAL_SECONDS=3600
      - REWRITE_WINDOW_DAYS=7

    # Networking
    networks:
      - helios-net

    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy
      llm-gemma3-4b:
        condition: service_started
      llm-embedding:
        condition: service_started

  # CLI Client Service (command line interface for single prompts)
  cli:
    build:
      context: .
      dockerfile: Dockerfile-cli
    container_name: helios-cli
    hostname: cli
    restart: "no" # Don't restart automatically
    volumes:
      - ipc_sockets:/tmp/ipc
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks:
      - helios-net
    depends_on:
      setup-ipc:
        condition: service_completed_successfully
      message-broker:
        condition: service_healthy
    environment:
      # System/Docker infrastructure config (overrides .env file defaults)
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1

  # Test Playback Service (validates audio setup without GPU)
  test-playback:
    build:
      context: .
      dockerfile: Dockerfile-test-playback
      cache_from:
        - ghcr.io/juno-ai-labs/juno-test-playback:latest
    container_name: helios-test-playback
    hostname: test-playback
    restart: "no" # Don't restart automatically - run once for testing

    # Audio device access
    devices:
      - /dev/snd:/dev/snd

    # Group permissions for audio
    group_add:
      - audio

    # Required for pulse audio
    user: "1000:1000"

    # Shared volumes with PulseAudio
    volumes:
      - /run/user/1000/pulse:/run/user/1000/pulse
      - /home/andromeda/.config/pulse:/home/andromeda/.config/pulse:ro
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro

    # Environment
    environment:
      # System/Docker infrastructure config
      - PYTHONPATH=/app:/app/shared
      - PYTHONUNBUFFERED=1
      # Pulse Audio
      - PULSE_SERVER=unix:/run/user/1000/pulse/native
      - XDG_RUNTIME_DIR=/run/user/1000
      - HOME=/home/andromeda

    # Networking
    networks:
      - helios-net

# Named volumes
volumes:
  # IPC communication via Unix domain sockets
  ipc_sockets:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: "size=100m,uid=1000,gid=1000,mode=0777"
  memory_data:
    driver: local

# Custom network for service discovery
networks:
  helios-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
